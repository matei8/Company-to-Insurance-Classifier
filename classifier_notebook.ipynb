{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insurance Classifier Notebook\n",
    "\n",
    "This notebook implements an insurance classification system that categorizes companies based on their descriptions using multiple NLP techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration and Analysis\n",
    "\n",
    "Before building our classifier, let's analyze the dataset to understand its characteristics:\n",
    "\n",
    "1. **Sector Distribution**:\n",
    "   - Shows the distribution of companies across different insurance sectors\n",
    "   - Helps identify class imbalance and data distribution\n",
    "   - Informs our classification strategy and threshold selection\n",
    "\n",
    "2. **Text Length Analysis**:\n",
    "   - Analyzes the distribution of words per sample\n",
    "   - Helps determine appropriate sequence lengths for our models\n",
    "   - Influences our feature processing decisions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_companies = pd.read_csv('ml_insurance_challenge.csv')\n",
    "\n",
    "# Plot sector distribution\n",
    "plt.figure(figsize=(10, 8))\n",
    "sector_counts = df_companies['sector'].value_counts()\n",
    "plt.bar(sector_counts.index, sector_counts.values)\n",
    "plt.xlabel('Insurance Sectors')\n",
    "plt.ylabel('Number of Companies')\n",
    "plt.title('Distribution of Companies Across Insurance Sectors')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and plot text length distribution\n",
    "samples_len = []\n",
    "cols = ['business_tags', 'category', 'niche']\n",
    "for chunk in pd.read_csv('ml_insurance_challenge.csv', usecols=cols, chunksize=1):\n",
    "    words_no = 0\n",
    "    tags = chunk['business_tags'].values[0]\n",
    "    category = chunk['category'].values[0]\n",
    "    niche = chunk['niche'].values[0]\n",
    "\n",
    "    if pd.notna(tags):\n",
    "        words_no += len(tags.split(' '))\n",
    "\n",
    "    if pd.notna(category):\n",
    "        words_no += len(category.split(' '))\n",
    "\n",
    "    if pd.notna(niche):\n",
    "        words_no += len(niche.split(' '))\n",
    "\n",
    "    samples_len.append(words_no)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.hist(samples_len, bins=50, edgecolor='black', density=False)\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Number of words per sample')\n",
    "plt.title(f'Distribution of words per sample)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average description length: {text_lengths.mean():.2f} words\")\n",
    "print(f\"Median description length: {text_lengths.median():.2f} words\")\n",
    "print(f\"Number of unique sectors: {len(sector_counts)}\")\n",
    "print(f\"Most common sector: {sector_counts.index[0]} ({sector_counts.values[0]} companies)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "First, we import all necessary Python libraries for data processing, machine learning, and deep learning. This includes:\n",
    "- Pandas for data manipulation\n",
    "- Scikit-learn for traditional ML models\n",
    "- PyTorch for neural networks\n",
    "- SentenceTransformers for text embeddings\n",
    "- Logging for tracking progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "import logging\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Separable CNN Model\n",
    "The SepCNN class implements a depthwise separable convolutional neural network for text classification. This architecture is more efficient than regular CNNs while maintaining good performance. Parameters:\n",
    "\n",
    "- `input_dim=768`: Matches BERT's hidden size (all-MiniLM-L6-v2 embeddings), ensuring compatibility with the sentence transformer\n",
    "- `kernel_size=(2, 3)`: Tuple for localizing 2-grams and 3-grams in the text\n",
    "- `padding=1`: Preserves sequence length after convolution, maintaining spatial information\n",
    "- `groups=input_dim`: Enables depthwise convolution, reducing parameters by processing each channel independently\n",
    "- `128 output channels`: Sufficient capacity for feature extraction while keeping model size manageable\n",
    "- `MaxPool1d(2)`: Reduces sequence length by half, capturing hierarchical features and reducing computation\n",
    "- `ReLU`: Standard non-linearity for deep learning, helps with gradient flow and sparsity\n",
    "\n",
    "The architecture follows a two-step process:\n",
    "1. Depthwise convolution extracts spatial features independently for each channel\n",
    "2. Pointwise convolution (1x1) mixes channels and projects to final feature space\n",
    "\n",
    "This design is particularly efficient for text classification because:\n",
    "- Reduces parameters by ~90% compared to regular CNN\n",
    "- Maintains good performance on insurance taxonomy classification\n",
    "- Works well with BERT embeddings (768 dimensions)\n",
    "- Handles variable-length text inputs through global average pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SepCNN(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(SepCNN, self).__init__()\n",
    "        self.separable_conv = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, input_dim, kernel_size=(2, 3), padding=1, groups=input_dim),\n",
    "            nn.Conv1d(input_dim, 128, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2)\n",
    "        )\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.separable_conv(x)\n",
    "        x = x.mean(dim=2)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Main InsuranceClassifier Class\n",
    "\n",
    "This is the core class that implements the insurance classification system. It:\n",
    "- Loads the taxonomy of insurance categories\n",
    "- Initializes multiple models (sentence transformer, MLP, SepCNN)\n",
    "- Sets up TF-IDF vectorizer to create more reliable scores for present expressions/words\n",
    "- Handles GPU acceleration if available\n",
    "\n",
    "Further, an instance of the classifier is created with a similarity threshold of 0.85 (less will generate false pozitives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsuranceClassifier:\n",
    "    def __init__(self, taxonomy_path: str, similarity_threshold: float = 0.8):\n",
    "        self.taxonomy_df = pd.read_csv(taxonomy_path)\n",
    "        self.taxonomy_labels = self.taxonomy_df['label'].tolist()\n",
    "        self.num_classes = len(self.taxonomy_labels)\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        \n",
    "        # Initialize models for quantifying the similarity between the company description and the taxonomy labels\n",
    "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.mlp_model = MLPClassifier(hidden_layer_sizes=(256, 128), max_iter=500)\n",
    "        self.sepcnn_model = SepCNN(768, self.num_classes)  # 768 is BERT base hidden size\n",
    "        \n",
    "        # Initialize vectorizers\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(\n",
    "            ngram_range=(2, 3),  # Bigrams AND Trigrams for more accuracy\n",
    "            min_df=2,\n",
    "            max_df=0.95,\n",
    "            stop_words='english'\n",
    "        )\n",
    "\n",
    "        # Check if CUDA is available\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.sepcnn_model = self.sepcnn_model.to(self.device)\n",
    "        \n",
    "        # Pre-compute taxonomy embeddings\n",
    "        self.taxonomy_embeddings = self.sentence_model.encode(self.taxonomy_labels)\n",
    "        logger.info(f\"Loaded {len(self.taxonomy_labels)} taxonomy labels\")\n",
    "\n",
    "# Initialize classifier with similarity threshold\n",
    "classifier = InsuranceClassifier('insurance_taxonomy.csv', similarity_threshold=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Similarity Matching Functions\n",
    "\n",
    "1. **Embedding Generation**:\n",
    "   - Uses `all-MiniLM-L6-v2` sentence transformer to convert text into 768-dimensional vectors\n",
    "   - Pre-computes embeddings for all taxonomy labels for efficiency\n",
    "   - Generates embeddings for company descriptions on-the-fly\n",
    "\n",
    "2. **Similarity Calculation**:\n",
    "   - Uses cosine similarity to measure semantic closeness between vectors\n",
    "   - Cosine similarity is preferred over Euclidean distance for text because:\n",
    "     - It's scale-invariant (focuses on angle between vectors)\n",
    "     - Better captures semantic similarity regardless of text length\n",
    "     - Ranges from -1 to 1, where 1 means identical meaning\n",
    "\n",
    "3. **Threshold-based Matching**:\n",
    "   - Default threshold of 0.85 to ensure high-quality matches\n",
    "   - Higher threshold reduces false positives but may miss some valid categories\n",
    "   - Lower threshold increases recall but may introduce noise\n",
    "\n",
    "4. **Fallback Mechanism**:\n",
    "   - If no matches meet the threshold, returns top 3 most similar categories\n",
    "   - Ensures every company gets at least some classification\n",
    "   - Helps handle edge cases and ambiguous descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_labels(InsuranceClassifier, text_embedding: np.ndarray) -> List[str]:\n",
    "    # Calculate similarities with all taxonomy labels which are above threshold\n",
    "    similarities = cosine_similarity([text_embedding], InsuranceClassifier.taxonomy_embeddings)[0]\n",
    "    \n",
    "    # Get indices of labels with similarity above threshold\n",
    "    similar_indices = np.where(similarities >= InsuranceClassifier.similarity_threshold)[0]\n",
    "    \n",
    "    # Get the corresponding labels\n",
    "    similar_labels = [InsuranceClassifier.taxonomy_labels[i] for i in similar_indices]\n",
    "    \n",
    "    # If no labels meet the threshold, get the top 3 most similar\n",
    "    if not similar_labels:\n",
    "        top_indices = np.argsort(similarities)[-3:]  # Get top 3 most similar\n",
    "        similar_labels = [InsuranceClassifier.taxonomy_labels[i] for i in top_indices]\n",
    "        logger.debug(f\"No labels above threshold {InsuranceClassifier.similarity_threshold}, using top 3: {similar_labels}\")\n",
    "    \n",
    "    return similar_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Processing Utilities\n",
    " \n",
    "Basic text processing methods:\n",
    "- `tokenize` is used for splitting text into words or characters\n",
    "- `clean_text` is used for normalization (lowercase, remove numbers/punctuation)\n",
    "- `prepare_data` it combines multiple text fields into one clean string. It writes it in a new column in the CSV file in order to use less memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str, mode: str = 'word') -> List[str]:\n",
    "    if mode == 'word':\n",
    "        return text.split()\n",
    "    return list(text)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    return ' '.join(text.split())\n",
    "\n",
    "def prepare_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['combined_text'] = (\n",
    "        df['description'].fillna('') + ' ' +\n",
    "        df['business_tags'].fillna('') + ' ' +\n",
    "        df['category'].fillna('') + ' ' +\n",
    "        df['niche'].fillna('')\n",
    "    ).apply(clean_text)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Processing Methods\n",
    "\n",
    "1. **N-gram Path (TF-IDF based)**:\n",
    "   - Uses bigrams and trigrams (`ngram_range=(2, 3)`)\n",
    "   - TF-IDF weighting:\n",
    "     - Downweights common terms across all documents\n",
    "     - Helps identify industry-specific language\n",
    "\n",
    "2. **Sequence Path (Embedding based)**:\n",
    "   - Leverages pre-trained sentence embeddings\n",
    "   - Captures semantic meaning and context\n",
    "   - Preserves word order and relationships\n",
    "   - Better for understanding complex descriptions\n",
    "\n",
    "3. **Feature Selection**:\n",
    "   - `select_top_k_features` with two modes:\n",
    "     - Score-based: Selects features with highest TF-IDF scores\n",
    "     - Frequency-based: Chooses most commonly occurring terms\n",
    "   - Improves model efficiency and generalization\n",
    "\n",
    "4. **Normalization Options**:\n",
    "   - Sample-wise: Normalizes each document independently\n",
    "   - Feature-wise: Normalizes across all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_top_k_features(features: np.ndarray, k: int, mode: str = 'score') -> np.ndarray:\n",
    "    if mode == 'score':\n",
    "        scores = np.sum(features, axis=0)\n",
    "        top_indices = np.argsort(scores)[-k:]\n",
    "        return features[:, top_indices]\n",
    "    else:  # freq mode\n",
    "        freq = np.sum(features > 0, axis=0)\n",
    "        top_indices = np.argsort(freq)[-k:]\n",
    "        return features[:, top_indices]\n",
    "\n",
    "def normalize_features(features: np.ndarray, mode: str = 'None') -> np.ndarray:\n",
    "    if mode == 'samplewise':\n",
    "        return normalize(features, norm='l2', axis=1)\n",
    "    elif mode == 'featurewise':\n",
    "        return normalize(features, norm='l2', axis=0)\n",
    "    return features\n",
    "\n",
    "def process_ngram_path(texts: List[str], top_k: int = 20000) -> np.ndarray:\n",
    "    # TF-IDF vectorization with bigrams\n",
    "    features = InsuranceClassifier.tfidf_vectorizer.fit_transform(texts).toarray()\n",
    "    \n",
    "    # Select top k features using f_classif scoring\n",
    "    features = select_top_k_features(features, min(top_k, features.shape[1]), mode='score')\n",
    "    return features\n",
    "\n",
    "def process_sequence_path(texts: List[str], top_k: int = 20000) -> np.ndarray:\n",
    "    # Get embeddings\n",
    "    embeddings = InsuranceClassifier.sentence_model.encode(texts)\n",
    "    \n",
    "    # Select top k features using frequency\n",
    "    embeddings = select_top_k_features(embeddings, min(top_k, embeddings.shape[1]), mode='freq')\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Classification and Evaluation\n",
    " \n",
    "**Company Classification (`classify_companies`)**:\n",
    "   - Processes company data through multiple stages:\n",
    "     - Text preparation and cleaning\n",
    "     - Embedding generation using sentence transformer\n",
    "     - Similarity matching with taxonomy labels\n",
    "     - Threshold-based assignment (default 0.85)\n",
    "     - Fallback to top 3 matches if needed\n",
    "   - Includes progress tracking and example logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_companies(ic: InsuranceClassifier, df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = prepare_data(df)\n",
    "    texts = df['combined_text'].tolist()\n",
    "    \n",
    "    # Get embeddings for all texts\n",
    "    logger.info(\"Computing embeddings for company descriptions...\")\n",
    "    text_embeddings = ic.sentence_model.encode(texts)\n",
    "    \n",
    "    # Get similar labels for each text\n",
    "    logger.info(\"Finding similar taxonomy labels...\")\n",
    "    all_labels = []\n",
    "    for i, embedding in enumerate(text_embeddings):\n",
    "        similar_labels = get_similar_labels(ic, embedding)\n",
    "        all_labels.append(', '.join(similar_labels))\n",
    "        \n",
    "        # Log progress every 100 companies\n",
    "        if (i + 1) % 100 == 0:\n",
    "            logger.info(f\"Processed {i + 1}/{len(texts)} companies\")\n",
    "    \n",
    "    df['insurance_label'] = all_labels\n",
    "    \n",
    "    # Log some examples\n",
    "    logger.info(\"\\nExample classifications:\")\n",
    "    for i in range(min(5, len(df))):\n",
    "        logger.info(f\"\\nCompany: {df.iloc[i]['description'][:100]}...\")\n",
    "        logger.info(f\"Assigned labels: {df.iloc[i]['insurance_label']}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Classify companies\n",
    "df_classified = classify_companies(classifier, df_companies)\n",
    "\n",
    "df_classified = df_classified.drop(columns=['combined_text']) # Drop intermediate column\n",
    "df_classified.to_csv('classified_companies.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
